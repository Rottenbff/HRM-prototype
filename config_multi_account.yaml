# Multi-Account Training Configuration
# Optimized for frequent checkpointing (every ~15-20 minutes)

defaults:
  - arch: trm
  - _self_

hydra:
  output_subdir: null

# Data path
data_paths: ['data/sudoku-extreme-1k-aug-1000']
data_paths_test: []

evaluators: []  # No evaluators for faster training

# Hyperparams - Training
global_batch_size: 768

epochs: 50000

# KEY SETTING: Save checkpoint every 500 steps (~15-20 minutes on H100)
eval_interval: 500
checkpoint_every_eval: True

# Early saving for multi-account convenience
save_interval: 500

lr: 1e-4
lr_min_ratio: 1.0
lr_warmup_steps: 2000

# Standard hyperparameter settings for LM, as used in Llama
beta1: 0.9
beta2: 0.95
weight_decay: 1.0  # Higher for sudoku
puzzle_emb_weight_decay: 1.0  # Higher for sudoku

# Hyperparams - Puzzle embeddings training
puzzle_emb_lr: 1e-4

seed: 0
min_eval_interval: 0  # when to start the eval

ema: True  # Use EMA for better stability
ema_rate: 0.999
freeze_weights: False

# Project naming for tracking
project_name: "sudoku_pretrain_multiaccount"
run_name: "rotating_accounts_v1"
